{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9GpVcbNknxW9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the dataset with encoded categorical features\n",
        "final_data = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Shakhtery/data-mining/main/Data/final_data.csv',\n",
        "    index_col=0\n",
        ")\n",
        "\n",
        "# read the dataset with plain categorical features\n",
        "final_data_raw = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Shakhtery/data-mining/main/Data/final_data_raw.csv',\n",
        "    index_col=0\n",
        ")"
      ],
      "metadata": {
        "id": "3tOK1XsWpAhf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### current code"
      ],
      "metadata": {
        "id": "icw6wdHhnWbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target variables are probabilities of the first and the second players\n",
        "targets = ['player_1_won', 'player_2_won']\n",
        "\n",
        "# include in predicting features all columns except date(already encoded in the dataset)\n",
        "# and data about odds\n",
        "predictors = list(set(final_data.columns).difference(set(targets + ['date', 'maxw', 'maxl', 'avgw', 'avgl'])))"
      ],
      "metadata": {
        "id": "t0Zh7fRBsouy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# convert date from str to datetime\n",
        "final_data['date'] = final_data['date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))"
      ],
      "metadata": {
        "id": "_eU9SjCwYa4a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort dataset by date\n",
        "final_data.sort_values('date', inplace=True, ignore_index=True)"
      ],
      "metadata": {
        "id": "CZc0eYT6oq9e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "def train_test_split(train_end_year):\n",
        "    train = final_data[final_data['date'].dt.year <= train_end_year]\n",
        "    test = final_data[final_data['date'].dt.year > train_end_year]\n",
        "    return (\n",
        "        train[predictors], # X_train\n",
        "        train[targets[0]], # Y_train\n",
        "        test[predictors], # X_test\n",
        "        test[targets[0]] # Y_test\n",
        "    )"
      ],
      "metadata": {
        "id": "yTzHKHRrX7kE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing function\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def model_score(model, metric):\n",
        "    scores = []\n",
        "    for train_end_year in tqdm(range(2000, 2020)):\n",
        "        X_train, Y_train, X_test, Y_test = train_test_split(train_end_year)\n",
        "        model.fit(X_train, Y_train)\n",
        "        if metric == 'accuracy':\n",
        "            score = accuracy_score(Y_test, model.predict(X_test))\n",
        "        elif metric == 'precision':\n",
        "            score = precision_score(Y_test, model.predict(X_test), average=None)\n",
        "        elif metric == 'recall':\n",
        "            score = recall_score(Y_test, model.predict(X_test), average=None)\n",
        "        elif metric == 'f1':\n",
        "            score = f1_score(Y_test, model.predict(X_test), average=None)\n",
        "        else:\n",
        "            raise Exception()\n",
        "        scores.append(score)\n",
        "    return np.mean(scores)"
      ],
      "metadata": {
        "id": "Ttqrfa1OwA6x"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### old code"
      ],
      "metadata": {
        "id": "G1czhPf0pokq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_predictors = list(set(final_data_raw.columns).difference({'player_1_won', 'player_2_won', 'maxw', 'maxl', 'avgw', 'avgl'}))\n",
        "\n",
        "# target variables are probabilities of the first and the second players\n",
        "target = ['player_1_won']"
      ],
      "metadata": {
        "id": "l90Xu3Cgpu1u"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# convert date from str to datetime\n",
        "final_data_raw['date'] = final_data_raw['date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))"
      ],
      "metadata": {
        "id": "RGMSHZDIqXzi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort dataset by date\n",
        "final_data_raw.sort_values('date', inplace=True, ignore_index=True)"
      ],
      "metadata": {
        "id": "qUnSvDizqdrb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "#def train_test_split(X: pd.DataFrame, y: pd.Series, X_date: pd.Series, train_end_year: int):\n",
        "def train_test_split(X: pd.DataFrame, y: pd.Series, train_end_year: int):\n",
        "    mask = X['date'].dt.year <= train_end_year\n",
        "    \n",
        "    # (X_train, y_train, X_test, Y_test)\n",
        "    return X[mask], y[mask], X[~mask], y[~mask]"
      ],
      "metadata": {
        "id": "xACols2bnU5K"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feature_engine"
      ],
      "metadata": {
        "id": "d-dPqVBgv0ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from feature_engine.creation import CyclicalFeatures\n",
        "\n",
        "# Label encoding\n",
        "# Location, winner_name/loser_name encode\n",
        "def label_encoding(df):\n",
        "    df[\"location\"] = df[\"location\"].astype('category').cat.codes\n",
        "\n",
        "    conc_names = pd.concat([df[\"player_1_name\"], df[\"player_2_name\"]]).astype('category').cat.codes\n",
        "    df[\"player_1_name\"] = conc_names.iloc[:len(conc_names)//2]\n",
        "    df[\"player_2_name\"] = conc_names.iloc[len(conc_names)//2:]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Date encode\n",
        "def encode_date(df):\n",
        "    df[\"year\"] = df[\"date\"].apply(lambda x: x.year)\n",
        "    df[\"month\"] = df[\"date\"].apply(lambda x: x.month)\n",
        "    df[\"day\"] = df[\"date\"].apply(lambda x: x.day)\n",
        "\n",
        "    cyclical = CyclicalFeatures(variables=None, drop_original=True)\n",
        "    df = pd.concat([df, cyclical.fit_transform(df[[\"year\", \"month\", \"day\"]])], axis=1)\n",
        "    df = df.drop([\"date\", \"year\", \"month\", \"day\"], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# One-hot encoding\n",
        "# Court, surface, round, best_of, tourney_level, winner_hand/loser_hand, winner_ioc/loser_ioc encoding\n",
        "def one_hot_encoding(df):\n",
        "    df = pd.concat([df, pd.get_dummies(df[\"court\"])], axis=1).drop(\"court\", axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df[\"surface\"])], axis=1).drop(\"surface\", axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df[\"round\"])], axis=1).drop(\"round\", axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df[\"best_of\"])], axis=1).drop(\"best_of\", axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df[\"tourney_level\"])], axis=1).drop(\"tourney_level\", axis=1)\n",
        "\n",
        "    ohe_conc_players_hand = pd.get_dummies(pd.concat([df[\"player_1_hand\"], df[\"player_2_hand\"]]))\n",
        "    df = pd.concat([df, ohe_conc_players_hand.iloc[:len(ohe_conc_players_hand)//2]], axis=1).drop(\"player_1_hand\", axis=1)\n",
        "    df = pd.concat([df, ohe_conc_players_hand.iloc[len(ohe_conc_players_hand)//2:]], axis=1).drop(\"player_2_hand\", axis=1)\n",
        "\n",
        "    ohe_conc_players_ioc = pd.get_dummies(pd.concat([df[\"player_1_ioc\"], df[\"player_2_ioc\"]]))\n",
        "    df = pd.concat([df, ohe_conc_players_ioc.iloc[:len(ohe_conc_players_ioc)//2]], axis=1).drop(\"player_1_ioc\", axis=1)\n",
        "    df = pd.concat([df, ohe_conc_players_ioc.iloc[len(ohe_conc_players_ioc)//2:]], axis=1).drop(\"player_2_ioc\", axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Standard Scaling\n",
        "# wrank/lrank, winner_ht/loser_ht, winner_age/loser_age scale\n",
        "def std_scale(series, mean, std):\n",
        "    return (series-mean)/std\n",
        "\n",
        "def scale(df):\n",
        "    concated = pd.concat([df[\"player_1_rank\"], df[\"player_2_rank\"]])\n",
        "    mean = concated.mean()\n",
        "    std = concated.std()\n",
        "    df[\"player_1_rank\"] = std_scale(df[\"player_1_rank\"], mean, std)\n",
        "    df[\"player_2_rank\"] = std_scale(df[\"player_2_rank\"], mean, std)\n",
        "\n",
        "    concated = pd.concat([df[\"player_1_ht\"], df[\"player_2_ht\"]])\n",
        "    mean = concated.mean()\n",
        "    std = concated.std()\n",
        "    df[\"player_1_ht\"] = std_scale(df[\"player_1_ht\"], mean, std)\n",
        "    df[\"player_2_ht\"] = std_scale(df[\"player_2_ht\"], mean, std)\n",
        "\n",
        "    concated = pd.concat([df[\"player_1_age\"], df[\"player_2_age\"]])\n",
        "    mean = concated.mean()\n",
        "    std = concated.std()\n",
        "    df[\"player_1_age\"] = std_scale(df[\"player_1_age\"], mean, std)\n",
        "    df[\"player_2_age\"] = std_scale(df[\"player_2_age\"], mean, std)\n",
        "\n",
        "    return df\n",
        "\n",
        "def column_names_to_str(df):\n",
        "    df.columns = df.columns.astype(str)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(data):\n",
        "    pipeline = [\n",
        "        label_encoding,\n",
        "        encode_date,\n",
        "        one_hot_encoding,\n",
        "        scale,\n",
        "        column_names_to_str\n",
        "    ]\n",
        "\n",
        "    result = data\n",
        "    for operation in pipeline:\n",
        "        result = operation(result)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "vzqtmIpJuCTh"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing function\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def model_score(model, metric):\n",
        "    scores = []\n",
        "    for train_end_year in tqdm(range(2000, 2020)):\n",
        "        X_train, y_train, X_test, y_test = train_test_split(\n",
        "            final_data_raw[raw_predictors],\n",
        "            final_data_raw[target],\n",
        "            train_end_year\n",
        "        )\n",
        "        X_train, X_test = preprocess_data(X_train), preprocess_data(X_test)\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        if metric == 'accuracy':\n",
        "            score = accuracy_score(y_test, model.predict(X_test))\n",
        "        elif metric == 'precision':\n",
        "            score = precision_score(y_test, model.predict(X_test), average=None)\n",
        "        elif metric == 'recall':\n",
        "            score = recall_score(y_test, model.predict(X_test), average=None)\n",
        "        elif metric == 'f1':\n",
        "            score = f1_score(y_test, model.predict(X_test), average=None)\n",
        "        else:\n",
        "            raise Exception()\n",
        "        \n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores)"
      ],
      "metadata": {
        "id": "mj5jnWZ2JNGY"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "v4rhoG-lrOqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Modeling Technique"
      ],
      "metadata": {
        "id": "9U0creLQrWbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "r3rNRafBR_AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "7bpTU_p_QPXT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    '\\n',\n",
        "    model_score(\n",
        "        LogisticRegression(solver='liblinear'),\n",
        "        metric='accuracy'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Sdf6_uuF-D",
        "outputId": "67697e08-5429-46b4-eaae-310242f9a426"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:30<00:00,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.619267644898964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    '\\n',\n",
        "    model_score(\n",
        "        LogisticRegression(solver='liblinear'),\n",
        "        metric='precision'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sdAohMUxEUX",
        "outputId": "113d0ae8-d360-48d1-ebbf-38e43a5c695c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:31<00:00,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.6195024685930328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    '\\n',\n",
        "    model_score(\n",
        "        LogisticRegression(solver='liblinear'),\n",
        "        metric='recall'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLsayM2qzLOt",
        "outputId": "f696d478-0157-46b4-ef3e-3c57ddc05570"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:33<00:00,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.6193808913264073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    '\\n',\n",
        "    model_score(\n",
        "        LogisticRegression(solver='liblinear'),\n",
        "        metric='f1'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usz5kgsp1ojA",
        "outputId": "a1014faf-3a7c-4feb-dd84-e5cf4db7c7d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:21<00:00,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.6191743298257294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fqHMRMpq3907"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}